# -*- coding: utf-8 -*-
"""PECINTA PADUAN KUDA NGAWI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19qq_-BOG527u_bIbalIsXqbeGmu7gLoR
"""

import requests
from bs4 import BeautifulSoup
import re
import os

import pandas as pd

URL= "https://horsedvm.com/views/health.php"
DISEASE_URL_PATTERN= r'disease/([^/]+)'
DISEASE_BASE_URL= "https://horsedvm.com/disease/"

response = requests.get(URL)

soup = BeautifulSoup(response.text, 'html.parser')
list_group = soup.find('div', class_='list-group')

# list_group

link_list= []

links = list_group.find_all('a')

for link in links:
    slug = link.get('href')
    match = re.search(DISEASE_URL_PATTERN, slug)

    if match:
        link_list.append(os.path.join(DISEASE_BASE_URL, match.group(1)).strip())
    else:
        print("No match found", link)

len(link_list)

def cleanse_scraped_text(text):
    # Hilangkan newline dan carriage return
    clean_text = re.sub(r'[\n\r]+', ' ', text)
    # Hapus spasi berlebih
    clean_text = re.sub(r'\s{2,}', ' ', clean_text)
    return clean_text.strip().lower()

disease_information_list= []
disease_unique_symptom_list= []

for idx, disease_url in enumerate(link_list):
    try:
        disease_response = requests.get(disease_url)
        disease_soup = BeautifulSoup(disease_response.text, 'html5lib')

        div_condition_details = disease_soup.find('div', id='conditions-details')

        h1_title = div_condition_details.find('h1', recursive=False)
        disease_name= h1_title.get_text(strip=True)

        div_row = disease_soup.find('div', class_='row')
        disease_category= div_row.find_all('a')[-1].get_text(strip=True)
        disease_category

        div_disease_desc = div_condition_details.find('div', class_='disease-desc')

        # Kasi ilang table (for now, nanti akan diproses)
        for table in div_disease_desc.find_all('table'):
            table.decompose()
        disease_desc= cleanse_scraped_text(div_disease_desc.get_text())

        div_symptom_list = disease_soup.find('div', id='sympGo')
        div_symptom_element = div_symptom_list.find_all('div', class_='symptom-element')
        temp_span_list = [child.find('span') for child in div_symptom_element]

        symptom_list= []
        for span in temp_span_list:
            if span:
                cleaned_span= span.get_text(strip=True).lower()
                symptom_list.append(cleaned_span)

                if cleaned_span not in disease_unique_symptom_list:
                    disease_unique_symptom_list.append(cleaned_span)

        # symptom_list= [span.get_text(strip=True).lower() for span in temp_span_list if span]
        disease_symptom_list = ', '.join(symptom_list)

        disease_information_dict= {
            "name": cleanse_scraped_text(disease_name),
            "category": cleanse_scraped_text(disease_category),
            "symptom_list": cleanse_scraped_text(disease_symptom_list),
            "description": cleanse_scraped_text(disease_desc),
            "url": disease_url
        }

        disease_information_list.append(disease_information_dict)

        print(f'Indeks-{idx} Success')

    except requests.exceptions.HTTPError as e:
        print(f"Indeks-{idx} HTTP Error: {e}, URL: {disease_url}")
    except requests.exceptions.ConnectionError as e:
        print(f"Indeks-{idx} Connection Error: {e}, URL: {disease_url}")
    except requests.exceptions.Timeout as e:
        print(f"Indeks-{idx} Timeout Error: {e}, URL: {disease_url}")
    except requests.exceptions.RequestException as e:
        print(f"Indeks-{idx} Request Error: {e}, URL: {disease_url}")
    except Exception as e:
        print(f"Indeks-{idx} Error saat scraping: , URL: {disease_url}{e}")

    # if idx>=3:
    #         break

df_disease= pd.DataFrame(disease_information_list)
df_disease

def row_to_doc(row):
    return {
        "id": str(row.name),
        "document": (
            f"Disease Name: {row['name']}\n"
            f"Category: {row['category']}\n"
            f"Symptoms: {row['symptom_list']}\n"
            f"Description: {row['description']}"
        ),
        "metadata": {
            "name": row["name"],
            "category": row["category"],
            "url": row["url"]
        }
    }

import json

docs = df_disease.apply(row_to_doc, axis=1).tolist()

with open("horse_diseases_docs.json", "w", encoding="utf-8") as f:
    json.dump(docs, f, ensure_ascii=False, indent=2)

print(f"Saved {len(docs)} documents to horse_diseases_docs.json")

df_disease.to_csv('horse_diseases.csv', index=None)